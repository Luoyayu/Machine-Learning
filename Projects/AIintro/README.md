本文件夹内为 2019年春人工智能导论课程设计作业

课题：Transformer上的注意力机制应用于机器翻译(de-en) 

指导老师：俞，詹

组长：

组员：



课题简述：

NLP 方向传统的方法依据 RNN或CNN等对序列进行表示计算，因序列固有的顺序性带来了并行计算的困难。Transformer 仅依靠注意力机制改善位置距离计算带来的复杂度，进一步提高了机器翻译等NLP任务的表现。

本课程设计力在复现《Attention Is All You Need》在机器翻译上的表现，并与传统的循环模型和卷积模型对照。

项目目录结构：





