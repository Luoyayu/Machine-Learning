{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\mathrm{TensorFlow}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run in Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = x*x*y+y+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session() # æ‰“å¼€ä¸€ä¸ªTensorFlowä¼šè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 False\n"
     ]
    }
   ],
   "source": [
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result, sess is tf.get_default_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close() # å…³é—­sessä¼šè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 42 42\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # withçš„Blockå—å†…æ‰“å¼€é»˜è®¤ä¼šè¯\n",
    "    x.initializer.run() # ç­‰ä»·äº tf.get_default_session().run(x.initializer)\n",
    "    y.initializer.run()\n",
    "    print(f.eval(), sess.run(f), tf.get_default_session().run(f))\n",
    "    print(sess is tf.get_default_session()) # True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# å…¨å±€å˜é‡åˆå§‹åŒ–å™¨\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(f.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 True\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession() # åœ¨jupyteræˆ–è€…Python Shellçš„ç¯å¢ƒä¸‹å¯ä»¥ä½¿ç”¨äº¤äº’å¼ä¼šè¯æ­¤æ—¶é»˜è®¤è¿˜æ˜¯defaultä¼šè¯\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result, sess is tf.get_default_session())\n",
    "sess.close() # äº¤äº’å¼ä¼šè¯åªèƒ½activeä¸€ä¸ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{TensorFlow}$åˆ©ç”¨å›¾çš„æ€æƒ³ï¼Œå°†æ‰€æœ‰æ“ä½œæŒ‚åœ¨èŠ‚ç‚¹ä¸Š, é»˜è®¤æ“ä½œå‡æŒ‚åœ¨é»˜è®¤å›¾ä¸Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph() # é»˜è®¤å›¾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manging Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "print(x2 is tf.get_default_graph()) # False ä¸æ˜¯é»˜è®¤å›¾ï¼Œx2æŒ‚åœ¨graphå›¾ä¸Š\n",
    "#æ³¨æ„åœ¨jupyterç¯å¢ƒä¸‹æ‰€æœ‰å…¶ä»–çš„å›¾ä¸Šçš„èŠ‚ç‚¹éƒ½ä¼šåœ¨é»˜è®¤å›¾ä¸Šäº§ç”Ÿæ‹·è´ï¼Œæ­¤æ—¶éœ€è¦ç»å¸¸resetå†…æ ¸æˆ–è€…\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifecycle of a Node Value\n",
    "$\\mathrm{TensorFlow}$çš„$node\\ value$çš„ç”Ÿå‘½å‘¨æœŸæ˜¯åœ¨ä¸€æ¬¡**run**çš„æœŸé—´   \n",
    "åœ¨å•ä¸ªå¤„ç†ä¸­çš„$\\mathrm{TensorFlow}$ä¸åŒä¼šè¯ä¸ä¼šå…±äº«çŠ¶æ€ï¼Œåªæœ‰åœ¨åˆ†å¸ƒå¼ä¸ŠçŠ¶æ€æ‰ä¼šå­˜åœ¨æœåŠ¡å™¨ä¸Šè¾¾æˆå…±äº«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.constant(5)\n",
    "x = w + 2 # 7\n",
    "y = x + 5 # 12\n",
    "z = x * 3 # 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 21\n",
      "12 21\n",
      "[12, 21]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(y.eval(), z.eval()) # å®é™…ä¸ŠTensorFlowåœ¨è®¡ç®—yåï¼Œä¼šä¸¢å¼ƒæ‰å›¾ä¸Šçš„èŠ‚ç‚¹å€¼ï¼Œé‡æ–°è®¡ç®—zä¾èµ–çš„å˜é‡\n",
    "    print(sess.run(y), sess.run(z))\n",
    "    print(sess.run([y, z]))   # è¿™æ ·åœ¨ä¸€æ¬¡runä¸­æ‰èƒ½é‡å¤åˆ©ç”¨èŠ‚ç‚¹å€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1) (3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]]), (3, 2))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1],[2],[3]])\n",
    "b = np.array([[4],[5],[6]])\n",
    "print(a.shape, b.shape)\n",
    "np.c_[a, b], np.c_[a, b].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale: 20640 8\n",
      "[[-3.7383255e+01]\n",
      " [ 4.3574786e-01]\n",
      " [ 9.3480907e-03]\n",
      " [-1.0648697e-01]\n",
      " [ 6.4317447e-01]\n",
      " [-4.2286665e-06]\n",
      " [-3.7746700e-03]\n",
      " [-4.2583770e-01]\n",
      " [-4.3952349e-01]] \n",
      "theta's scale: (9, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "print(\"scale:\", m, n)\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y) # é—­è§£\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    print(theta_value, '\\ntheta\\'s scale:', theta_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.6671883e+01]\n",
      " [ 4.3290150e-01]\n",
      " [ 9.2395172e-03]\n",
      " [-1.0443940e-01]\n",
      " [ 6.4053464e-01]\n",
      " [-7.9397687e-06]\n",
      " [-3.9392635e-03]\n",
      " [-4.1903567e-01]\n",
      " [-4.3165210e-01]]\n",
      "é¢„æµ‹:\n",
      " [[2.461358  ]\n",
      " [0.34167862]\n",
      " [2.1159124 ]\n",
      " ...\n",
      " [1.3530731 ]\n",
      " [0.7720909 ]\n",
      " [2.8207245 ]] \n",
      "çœŸå®æ ‡è®°:\n",
      " [[4.889]\n",
      " [1.078]\n",
      " [1.503]\n",
      " ...\n",
      " [1.276]\n",
      " [1.01 ]\n",
      " [4.857]] \n",
      "å‡æ–¹è¯¯å·®: 0.36666975971019805 0.5377607\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "data = housing.data\n",
    "\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), data]\n",
    "X_train, X_val, y_train, y_val = train_test_split(housing_data_plus_bias, housing.target.reshape(-1,1))\n",
    "\n",
    "X_train = tf.constant(X_train, dtype=tf.float32, name='X_train')\n",
    "X_val = tf.constant(X_val, dtype=tf.float32, name='X_val')\n",
    "\n",
    "y_train = tf.constant(y_train, dtype=tf.float32, name='y_train')\n",
    "y_val = tf.constant(y_val, dtype=tf.float32, name='y_val')\n",
    "\n",
    "X_train_T = tf.transpose(X_train)\n",
    "\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(X_train_T, X_train)), X_train_T), y_train)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    print(theta_value)\n",
    "    y_pred = tf.matmul(X_val, theta_value)\n",
    "    error = y_pred - y_val\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "    print('é¢„æµ‹:\\n', y_pred.eval(),'\\nçœŸå®æ ‡è®°:\\n', y_val.eval(),'\\nå‡æ–¹è¯¯å·®:',np.sqrt(np.sum(error.eval()**2)/(m-1)), mse.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  20640 * 8\n",
      "0.11111111111111005\n",
      "(15480, 9) (15480, 1) (5160, 9) (5160, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "data = housing.data\n",
    "m, n = data.shape\n",
    "print('data shape: ', m, '*', n)\n",
    "target = housing.target\n",
    "scaled_data =  StandardScaler().fit_transform(data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m,1)), scaled_data]\n",
    "print(np.mean(scaled_housing_data_plus_bias))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(scaled_housing_data_plus_bias, target.reshape(-1,1))\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "# æ³¨æ„Tensorä¸æ˜¯å ä½ç¬¦çš„åˆæ³•æ›¿ä»£ï¼Œç›´æ¥ä½¿ç”¨np.arrayå³å¯\n",
    "# X_train = tf.constant(X_train, dtype=tf.float32, name='X_train')\n",
    "# X_val   = tf.constant(X_val,   dtype=tf.float32, name='X_val')\n",
    "# y_train = tf.constant(y_train, dtype=tf.float32, name='y_train')\n",
    "# y_val   = tf.constant(y_val,   dtype=tf.float32, name='y_val')\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None, 9),  name=\"X\")\n",
    "Y = tf.placeholder(tf.float32,shape=(None, 1), name=\"Y\")\n",
    "Y_Val = tf.placeholder(tf.float32,shape=(None, 1), name=\"Y_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'theta:0' shape=(9, 1) dtype=float32_ref>\n",
      "Epoch 0000 <==============> MES = 3.9040933\n",
      "Epoch 0100 <==============> MES = 0.6593812\n",
      "Epoch 0200 <==============> MES = 0.5448185\n",
      "Epoch 0300 <==============> MES = 0.5353779\n",
      "Epoch 0400 <==============> MES = 0.5323018\n",
      "Epoch 0500 <==============> MES = 0.53024447\n",
      "Epoch 0600 <==============> MES = 0.52868503\n",
      "Epoch 0700 <==============> MES = 0.5274715\n",
      "Epoch 0800 <==============> MES = 0.5265194\n",
      "Epoch 0900 <==============> MES = 0.5257696\n",
      "Epoch 1000 <==============> MES = 0.52517927\n",
      "Epoch 1100 <==============> MES = 0.5247141\n",
      "val_mse = 0.5331955\n",
      "best_theta: [ 2.0635517e+00  8.2997817e-01  1.2403106e-01 -2.5593805e-01\n",
      "  2.6323408e-01 -1.6935226e-03 -4.8034508e-02 -7.9701191e-01\n",
      " -7.6936400e-01]\n",
      "CPU times: user 2.29 s, sys: 169 ms, total: 2.46 s\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 1200\n",
    "lr = 0.01\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error =  y_pred - Y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = 2 / m * tf.matmul(tf.transpose(X), error)\n",
    "training_ops = tf.assign(theta, theta - lr * gradients) # geadient descent: theta = theta - lr * gradients\n",
    "init = tf.global_variables_initializer()\n",
    "print(theta)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", str(epoch).zfill(4), '<==============> MES =', mse.eval(feed_dict={X:X_train, Y:y_train}))\n",
    "        training_ops.eval(feed_dict={X:X_train, Y:y_train})\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "    print('val_mse =', mse.eval(feed_dict={X:X_val, Y:y_val}))\n",
    "print('best_theta:', best_theta.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff\n",
    "$\\mathrm{TensorFlow}$ä½¿ç”¨$\\mathrm{Reverse-mode\\ autodiff}$è‡ªåŠ¨åŒ–æ±‚æ¢¯åº¦, åŸç†è§é™„å½•D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640 8\n",
      "0.11111111111111005\n",
      "(15480, 9) (15480, 1) (5160, 9) (5160, 1)\n",
      "Epoch-0000 <==============> MES = 13.142555236816406\n",
      "Epoch-0100 <==============> MES = 0.8662416338920593\n",
      "Epoch-0200 <==============> MES = 0.6112032532691956\n",
      "Epoch-0300 <==============> MES = 0.5796080827713013\n",
      "Epoch-0400 <==============> MES = 0.5628477931022644\n",
      "Epoch-0500 <==============> MES = 0.5513385534286499\n",
      "Epoch-0600 <==============> MES = 0.5431396961212158\n",
      "Epoch-0700 <==============> MES = 0.5372486114501953\n",
      "Epoch-0800 <==============> MES = 0.5330027937889099\n",
      "Epoch-0900 <==============> MES = 0.5299386978149414\n",
      "Epoch-1000 <==============> MES = 0.5277246236801147\n",
      "Epoch-1100 <==============> MES = 0.5261224508285522\n",
      "val_mse = 0.5367589\n",
      "best_theta: [ 2.0632253   0.8093559   0.13925464 -0.18233489  0.21208344  0.00451868\n",
      " -0.05332402 -0.7908571  -0.7553287 ]\n",
      "CPU times: user 3.97 s, sys: 285 ms, total: 4.26 s\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "data = housing.data\n",
    "m, n = data.shape\n",
    "print(m, n)\n",
    "target = housing.target\n",
    "scaled_data =  StandardScaler().fit_transform(data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m,1)), scaled_data]\n",
    "print(np.mean(scaled_housing_data_plus_bias))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(scaled_housing_data_plus_bias, target.reshape(-1,1))\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "n_epochs = 1200\n",
    "lr = 0.01\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name='theta')\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "# gradients = tf.gradients(mse, [theta])[0] # è‡ªåŠ¨æ±‚åå¯¼ï¼Œè¾“å…¥å‡½æ•°ï¼Œã€å¾…æ±‚åå¯¼çš„å˜é‡åˆ—è¡¨ã€‘-ã€‹åˆ—è¡¨\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr) # TensorFlow è‡ªå¸¦çš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨\n",
    "# optimizer = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9) # åŠ¨é‡ä¼˜åŒ–æ¢¯åº¦ä¸‹é™\n",
    "training_ops = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch-{} <==============> MES = {}\".format(str(epoch).zfill(4) ,mse.eval(feed_dict={X:X_train, y:y_train})))\n",
    "        sess.run(training_ops, feed_dict={X:X_train, y:y_train})\n",
    "    best_theta = theta.eval()\n",
    "    print('val_mse =', mse.eval(feed_dict={X:X_val, y:y_val}))\n",
    "    print('best_theta:', best_theta.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Mini-batch GD\n",
    "ä½¿ç”¨å ä½ç¬¦$(placeholder)$ æ¥$pass$è®­ç»ƒæ•°æ®æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [2. 3. 4.]]\n",
      "\n",
      "[[6. 7. 8.]\n",
      " [7. 8. 9.]]\n",
      "\n",
      "[[ 6.  7.  8.]\n",
      " [ 7.  8.  9.]\n",
      " [ 8.  9. 10.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3)) # ç¬¬ä¸€ç»´åº¦ä»»æ„å¡«å……\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    A_val = A.eval(feed_dict={A:[[1,2,3], [2,3,4]]}) # can feed any ops not only placeholds\n",
    "    B_val_1 = B.eval(feed_dict={A:A_val})\n",
    "    B_val_2 = B.eval(feed_dict={A:[[1,2,3], [2,3,4], [3,4,5]]})\n",
    "print(A_val, B_val_1, B_val_2, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to implementing mini-batch GD modify X_train, y_train by placeholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_mse = 0.5199688\n",
      "best_theta: [ 2.0752447   0.8302172   0.11966775 -0.32165644  0.19795075 -0.00899185\n",
      " -0.02732604 -0.91518587 -0.88679576]\n",
      "CPU times: user 35.8 s, sys: 4.63 s, total: 40.4 s\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "data = housing.data\n",
    "target = housing.target\n",
    "m, n = data.shape\n",
    "\n",
    "scaled_data = StandardScaler().fit_transform(data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_data]\n",
    "\n",
    "X_train_tot, X_val, y_train_tot, y_val = train_test_split(scaled_housing_data_plus_bias, target.reshape(-1, 1))\n",
    "\n",
    "# X_val = tf.constant(X_val, dtype=tf.float32, name='X_val')\n",
    "# y_val = tf.constant(y_val, dtype=tf.float32, name='y_val')\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name='X')  # placeholders\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "n_epochs = 100\n",
    "lr = 0.01\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):  # éšæœºç”Ÿæˆbatch\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # ç”Ÿæˆä¸åŒçš„ç§å­\n",
    "    indices = np.random.randint(15480, size=batch_size)\n",
    "    X_batch = X_train_tot[indices]\n",
    "    y_batch = y_train_tot.reshape(-1,1)[indices]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "# gradients = tf.gradients(mse, [theta])[0]  # autodiff auto cal gradients\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)  # Use TensorFlow GD optimizer\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver() # ä¿å­˜èŠ‚ç‚¹\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            if epoch % 100 == 0:\n",
    "                save_path = saver.save(sess, './tmp/my_model.ckpt')\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "    y_val_pred = y_pred.eval(feed_dict={X:X_val})\n",
    "    val_mse = mse.eval(feed_dict={y_pred:y_val_pred, y:y_val})\n",
    "#     y_val_pred = tf.matmul(X_val, best_theta, name='y_val_pred')\n",
    "#     mse = tf.reduce_mean(tf.square(y_val_pred - y_val), name='mse')\n",
    "    print('val_mse =', val_mse)\n",
    "    print('best_theta:', best_theta.flatten())\n",
    "    save_path = saver.save(sess, './tmp/my_final_model.ckpt')\n",
    "    # saver.restore(sess, './tmp/my_final_model.ckpt')\n",
    "    # saver = tf.train.Saver({'weigh': theta}) # ä»¥å­—å…¸å½¢å¼ä¿å­˜ç‰¹å®šèŠ‚ç‚¹å˜é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/my_final_model.ckpt\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './tmp/my_final_model.ckpt')\n",
    "    restored_best_theta = theta.eval()\n",
    "    print(np.allclose(best_theta, restored_best_theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run in 2019_03_01_11_37_30\n",
      "val mse: 0.4936723\n",
      "Finish! ğŸ‘Œ\n",
      "CPU times: user 24.7 s, sys: 2.27 s, total: 27 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "data = housing.data\n",
    "target = housing.target\n",
    "m, n = data.shape\n",
    "\n",
    "scaled_data = StandardScaler().fit_transform(data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_data]\n",
    "\n",
    "X_train_tot, X_val, y_train_tot, y_val = train_test_split(scaled_housing_data_plus_bias, target.reshape(-1, 1))\n",
    "\n",
    "now = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "print('Run in', now)\n",
    "root_logdir = './tmp/tf_logs'\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "tf.reset_default_graph() # ç”Ÿæˆæ—¥å¿—æ—¶ä¸€å®šè¦å…ˆæ¸…é™¤é»˜è®¤å›¾\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "lr = 0.01\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "with tf.name_scope('loss') as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# gradients = 2 / m * tf.matmul(tf.transpose(X), error)\n",
    "# training_op = tf.assign(theta, theta - lr * gradients) # geadient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar(name='MSE', tensor=mse) # åˆ›å»ºæ ‡é‡çš„æ—¥å¿—èŠ‚ç‚¹\n",
    "file_writer = tf.summary.FileWriter(logdir=logdir, graph=tf.get_default_graph()) # åˆ›å»ºå†™æ–‡ä»¶å¤¹çš„èŠ‚ç‚¹\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):  # éšæœºç”Ÿæˆbatch\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # ç”Ÿæˆä¸åŒçš„ç§å­\n",
    "    indices = np.random.randint(15480, size=batch_size)\n",
    "    X_batch = X_train_tot[indices]\n",
    "    y_batch = y_train_tot.reshape(-1,1)[indices]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)                                       \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step) # è¿½å†™æ—¥å¿—+è®­ç»ƒæ­¥æ•°\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "    y_val_pred = y_pred.eval(feed_dict={X:X_val})\n",
    "    val_mse = mse.eval(feed_dict={y_pred:y_val_pred, y:y_val})\n",
    "    print(\"val mse:\", val_mse)\n",
    "    file_writer.close()\n",
    "    print('Finish! ğŸ‘Œ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨TensorFlowæ¨¡å—åŒ–æ“ä½œï¼Œè®¾è®¡äº”ä¸ªæ•´æµçº¿æ€§å•å…ƒ$rectified\\ linear\\ units(ReLU)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.860965]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "    b = tf.Variable(0.0, name='bias')\n",
    "    z = tf.add(tf.matmul(X, w), b, name='z')\n",
    "    return tf.maximum(z, 0.0, name='relu')\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name='output')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(output.eval(feed_dict={X: np.array([[1,2,3]])}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing Variables\n",
    "1. ä½¿ç”¨å˜é‡ç©ºé—´ï¼Œåˆ›å»ºç‰¹å®šç©ºé—´çš„å˜é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu/threshold\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope('relu'):\n",
    "    threshold = tf.get_variable('threshold', shape=(), initializer=tf.constant_initializer(0.0))\n",
    "    print(threshold.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu/threshold\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('relu', reuse=True): # ä½¿ç”¨tf.variable_scope reuse=True åˆ›å»ºå¯é‡ç”¨å˜é‡ç©ºé—´\n",
    "    threshold = tf.get_variable(\"threshold\")\n",
    "    # é»˜è®¤ä¸€ä¸ªå˜é‡ä¸å¯reuseé™¤éæŒ‡å…¶ set reuse=True or reuse=tf.AUTO_REUSE in VarScope\n",
    "print(threshold.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu/threshold\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('relu') as scope: # æˆ–è€…ç”¨reuse_variableså±æ€§æ‰“å¼€å¯é‡ç”¨\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")\n",
    "    print(threshold.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†reluå˜é‡ç©ºé—´ä¸­thresholdä¼ ç»™å¤šä¸ªReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "    b = tf.Variable(0.0, name='bias')\n",
    "    z = tf.add(tf.matmul(X, w), b, name='z')\n",
    "    with tf.variable_scope('relu', reuse=True):\n",
    "        threshod = tf.get_variable(\"threshold\") # è°ƒç”¨reluåŒºåŸŸçš„ å…±äº«å˜é‡\n",
    "    return tf.maximum(z, threshold, name='max')\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "\n",
    "with tf.variable_scope('relu'):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™æ ·çš„å±æ€§çœ‹èµ·æ¥ä¸å±äºReLU, æ‰€ä»¥å¯ä»¥åœ¨ä¸€ä¸ªæ•´æµå™¨ä¸Šå»ºç«‹å˜é‡ï¼Œåœ¨å…¶ä»–æ•´æµå™¨ä¸Šå…±äº«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "    b = tf.Variable(0.0, name='bias')\n",
    "    z = tf.add(tf.matmul(X, w), b, name='z')\n",
    "    threshold = tf.get_variable('threshold', shape=(), initializer=tf.constant_initializer(0.0)) # åˆæ¬¡åˆ›å»ºï¼Œå…¶ä»–reuseä¸å¿…å†æ¬¡ç”³æ˜shapeç­‰\n",
    "    return tf.maximum(z, threshold, name='max')\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "relus = []\n",
    "for relu_idx in range(5):\n",
    "    with tf.variable_scope('relu', reuse=(relu_idx >= 1)) as scope: # ç¬¬ä¸€æ¬¡åˆ›å»ºå˜é‡ï¼Œå…¶ä½™æ•´æµå™¨reuse=True\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "a_val = tf.Variable(5)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(a_val.eval(), sess.run(a_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3446319\n",
      "5.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(tf.random_uniform(shape=(), maxval=1.0, minval=0.0))\n",
    "x_new_val = tf.placeholder(shape=(), dtype=tf.float32)\n",
    "x_assign = tf.assign(x, x_new_val)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(x.initializer)\n",
    "    print(x.eval())\n",
    "#     x_assign.eval(feed_dict={x_new_val:5.5})\n",
    "    sess.run(x_assign, feed_dict={x_new_val:5.5})\n",
    "    print(x.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle\n",
      "zope.interface\n",
      "wxPython\n",
      "wheel\n",
      "Werkzeug\n",
      "webencodings\n",
      "wcwidth\n",
      "Twisted\n",
      "traitlets\n",
      "tornado\n",
      "torchvision\n",
      "torch\n",
      "toolz\n",
      "testpath\n",
      "terminado\n",
      "termcolor\n",
      "tensorflow\n",
      "tensorflow-probability\n",
      "tensorboard\n",
      "six\n",
      "sip\n",
      "simplegeneric\n",
      "setuptools\n",
      "service-identity\n",
      "Send2Trash\n",
      "scipy\n",
      "scikit-learn\n",
      "scikit-image\n",
      "pyzmq\n",
      "PyYAML\n",
      "PyWavelets\n",
      "pytz\n",
      "python-dateutil\n",
      "pyparsing\n",
      "pyOpenSSL\n",
      "Pygments\n",
      "pycparser\n",
      "pycodestyle\n",
      "pyasn1\n",
      "pyasn1-modules\n",
      "ptyprocess\n",
      "psutil\n",
      "protobuf\n",
      "prompt-toolkit\n",
      "prometheus-client\n",
      "pip\n",
      "Pillow\n",
      "pickleshare\n",
      "pexpect\n",
      "parso\n",
      "pandocfilters\n",
      "pandas\n",
      "olefile\n",
      "numpy\n",
      "notebook\n",
      "nose\n",
      "networkx\n",
      "nbformat\n",
      "nbconvert\n",
      "mkl-random\n",
      "mkl-fft\n",
      "mistune\n",
      "metakernel\n",
      "memory-profiler\n",
      "matplotlib\n",
      "matlabengineforpython\n",
      "matlab-kernel\n",
      "MarkupSafe\n",
      "Markdown\n",
      "line-profiler\n",
      "kiwisolver\n",
      "Keras\n",
      "Keras-Preprocessing\n",
      "Keras-Applications\n",
      "jupyterlab\n",
      "jupyterlab-server\n",
      "jupyterlab-launcher\n",
      "jupyter-core\n",
      "jupyter-client\n",
      "jsonschema\n",
      "Jinja2\n",
      "jedi\n",
      "ipython\n",
      "ipython-genutils\n",
      "ipykernel\n",
      "incremental\n",
      "imageio\n",
      "idna\n",
      "hyperlink\n",
      "html5lib\n",
      "h5py\n",
      "grpcio\n",
      "gast\n",
      "entrypoints\n",
      "decorator\n",
      "dask\n",
      "cytoolz\n",
      "cycler\n",
      "cryptography\n",
      "constantly\n",
      "cloudpickle\n",
      "cffi\n",
      "certifi\n",
      "bleach\n",
      "backports.lzma\n",
      "backcall\n",
      "autopep8\n",
      "Automat\n",
      "attrs\n",
      "astor\n",
      "asn1crypto\n",
      "appnope\n",
      "appdirs\n",
      "absl-py\n",
      "RunSnakeRun\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "from pip._internal.utils.misc import get_installed_distributions\n",
    "\n",
    "for dist in get_installed_distributions():\n",
    "    print(dist.project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
