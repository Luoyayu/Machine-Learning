{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$   \n",
    "$\\hat{y}=h_\\theta(\\mathbf{x})=\\theta^T \\cdot \\mathbf{x}$  \n",
    "$\\displaystyle MSE(\\theta)=MES(\\mathbf{X}, \\theta)=\\frac{1}{m}\\sum_{i=1}^m(\\theta^T \\cdot \\mathbf{x}^{(i)}-y^{(i)})^2$   \n",
    "闭解:  $\\hat{\\theta}=(\\mathbf{X}^T\\cdot \\mathbf{X}^{-1})\\cdot \\mathbf{X}^T\\cdot \\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "X = np.random.rand(100,1) * 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Models\n",
    "正则化线性模型有效避免模型过拟合, 通过约束权重有三种方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression\n",
    "岭回归   \n",
    "cost function 加二范数罚, 超参$\\alpha$控制, $\\alpha$越大约束越大  \n",
    "$\\displaystyle J(\\theta)=MSE(\\theta)+\\frac{1}{2}\\alpha\\sum_{i=1}^n\\theta_i^2$    \n",
    "$\\theta_0$不用正则,$\\frac{1}{2}$便于求导    \n",
    "在采用岭回归前要先缩放数据, 考虑在$l_2-ball$内一致性缩放   \n",
    "岭回归牺牲了无偏性提高了有效性, 减少了方差增加了偏差 more flatter(resonable)     \n",
    "闭解:$\\hat{\\theta}=(\\mathbf{X}^T\\cdot \\mathbf{X}+\\alpha \\mathbf{A})^{-1}\\cdot \\mathbf{X}^T \\cdot \\mathbf{y}$   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "    normalize=False, random_state=None, solver='cholesky', tol=0.001),\n",
       " SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "        fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "        loss='squared_loss', max_iter=None, n_iter=None, penalty='l2',\n",
       "        power_t=0.25, random_state=None, shuffle=True, tol=None, verbose=0,\n",
       "        warm_start=False))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, SGDRegressor\n",
    "\n",
    "ridge_reg = Ridge(alpha=1, solver='cholesky')\n",
    "sgd_reg = SGDRegressor(penalty='l2')\n",
    "ridge_reg, sgd_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression\n",
    "最小绝对收缩和选择算子    \n",
    "$cost\\ function$加一范数罚, 系数约束, 过高的degree可能在Lasso中权置零，进而消除不重要的特征     \n",
    "$\\displaystyle J(\\theta)=MSE(\\theta)+\\alpha\\sum_{i=1}^n|\\theta_i|$          \n",
    "在$0$处不可微, 引入$subgradient\\ vector\\ \\mathbf{g}$    \n",
    "$sign(\\theta_i)=\\begin{cases}\n",
    " -1 & \\text{ if } \\theta_i<0 \\\\ \n",
    " 0 & \\text{ if } \\theta_i=0 \\\\ \n",
    " +1 & \\text{ if } \\theta_i>0 \n",
    "\\end{cases}$   \n",
    "$g(\\theta, J)=\\bigtriangledown_\\theta MSE(\\theta)+\\alpha\\cdot sign(\\theta)$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Net\n",
    "弹性网络  \n",
    "$\\displaystyle J(\\theta)=MSE(\\theta)+r\\alpha\\sum_{i=1}^n|\\theta_i|+\\frac{1-r}{2}\\alpha\\sum_{i=1}^n\\theta_i^2, r \\in [0,1]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "何时使用这三种正则化呢?\n",
    "默认选Ridge, 如果发现只有少部分特征是有用的优先选择Elastic Net, Lasso表现不正常当特征数大于训练数,或者有些特征关联性十分强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earning Stopping\n",
    "停止法, 早停止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Logistic\\ Regression\\ model\\ estimated\\ probabilities$       \n",
    "$\\hat{p}=h_{\\theta}(\\mathbf{x})=\\sigma(\\theta^{T}\\cdot \\mathbf{x})\\in[0,1]$   \n",
    "<br>     \n",
    "- $Logistic\\ function$     \n",
    "$\\displaystyle \\sigma(t)=\\frac{1}{1+e^{-t}}$   \n",
    "<img src='./images/logisticfunction.png' align='left' style=' width:200px;height:100 px'/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Logistic\\ Regression\\ model\\ prediction$  \n",
    "$\n",
    "\\hat{y}=\n",
    "\\begin{cases}\n",
    " 0 & \\text{if }\\ \\hat{p}<0.5 \\\\\n",
    " 1 & \\text{if }\\ \\hat{p}\\geqslant0.5\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Cots Function\n",
    "训练模型和为$logistics\\ regression$确定损失函数   <br>  \n",
    "我们设计的损失函数要有以下的特性：接受$\\hat{p}$作为输入，对于$\\theta_{i}$, 以$x^{(i)}=\\mathbf{x}$为例     \n",
    "当标记为$1$时，$\\hat{p}$靠近$1$, $c(\\theta_i)$越小，远离$1$时, $c(\\theta_i)$越大   \n",
    "当标记为$0$时，相反  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from math import *\n",
    "plt.subplot(221);\n",
    "x = np.arange(0.00001, 1.5, 0.00001)\n",
    "y = -np.log(x)\n",
    "plt.title(r'$y=-log(\\hat{p})$')\n",
    "plt.plot(x, y)\n",
    "plt.hlines(0, 0, 1.5, colors = \"r\", linestyles = \"dashed\",lw = 0.5)\n",
    "plt.vlines(0, 0, 15, colors = \"r\", linestyles = \"dashed\",lw = 0.5)\n",
    "plt.plot([1.0],[0.0], '*')\n",
    "plt.subplot(222);\n",
    "plt.title(r'$y=-log(1-\\hat{p})$')\n",
    "y = -np.log(1-x)\n",
    "plt.plot(x, y)\n",
    "plt.hlines(0, 0, 1.5, colors = \"r\", linestyles = \"dashed\",lw = 0.5)\n",
    "plt.vlines(0, 0, 15, colors = \"r\", linestyles = \"dashed\",lw = 0.5)\n",
    "plt.plot([1.0],[0.0], '*')\n",
    "# plt.savefig('neglog.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/neglog.png' align='left'/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上图可得到单样例的损失函数\n",
    "$c(\\theta)=\n",
    "\\begin{cases} \n",
    "-log(\\hat{p}) &\\text{if }\\ \\mathrm{y}=1\\\\ \n",
    "-log(1-\\hat{p}) &\\text{if }\\ \\mathrm{y}=0\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "据$f=-log$函数的这种特性我们可以得出出在全体样例上的损失函数   \n",
    "<br>\n",
    "- $Logistic\\ Regression\\ cost\\ function\\ (log\\ loss)$   \n",
    "$\\displaystyle J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^m(\\hat{y}^{(i)}log(\\hat{p}^{(i)})+(1-\\hat{y}^{(i)})log(1-\\hat{p}^{(i)}))$   \n",
    "*Tips:这恰好是最小化$\\mathrm{cross\\ entropy}$*    \n",
    "注意该损失函数无法求出闭解,但是该函数是凸函数, 可以用*GD*或者其他算法求得全局最小值   \n",
    "<br>\n",
    "- $Logistics\\ cost\\ function\\ partial\\ derivatives$   \n",
    "有$\\hat{p}^{(i)}=\\sigma(\\theta^{T}\\cdot \\mathbf{x}^{(i)})$  \n",
    "带入上式得$\\displaystyle J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^m\\left [\\hat{y}^{(i)}\\sigma(\\theta^{T}\\cdot \\mathbf{x}^{(i)})+(1-\\hat{y}^{(i)})(1-\\sigma(\\theta^{T}\\cdot \\mathbf{x}^{(i)}))  \\right ]$    \n",
    "对$\\theta_j$求偏导数$\\displaystyle \\frac{\\partial }{\\partial \\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma(\\theta^T\\cdot \\mathbf{x}^{(i)})-y^{(i)})\\ x_j^{(i)}$    \n",
    "该形式与$MSE$的偏导数一致,很方便在一次迭代中求得所有的$\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundaries\n",
    "决策边界, 当$\\sigma(\\cdot)=0.5$时出现决策边界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'target', 'target_names', 'DESCR', 'feature_names']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris['data'][:, 3:]\n",
    "y = (iris['target']==2).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.50011773]\n"
     ]
    }
   ],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "y_proba[:]\n",
    "print(log_reg.predict_proba([[1.6129]])[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(X[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_proba[:,1], 'g--', label=\"pred Iris-V\")\n",
    "plt.plot(X_new, y_proba[:,0], 'b--', label=\"pred not Iris-V\")\n",
    "plt.plot(X[100:], np.ones(len(X[100:])), label='train is V')\n",
    "plt.plot(X[:100], np.zeros(len(X[:100])), label='train not V')\n",
    "plt.hlines(0,-0.1,3,colors = \"r\", linestyles = \"dashed\",lw = 0.5)\n",
    "plt.hlines(1,0,3,colors = \"r\", linestyles = \"dashed\",lw = 0.5)\n",
    "plt.vlines(0,-0.1,1,colors = \"r\", linestyles = \"dashed\",lw = 0.5)\n",
    "plt.vlines(1.6129, 0, 0.5,colors = \"r\", linestyles = \"dashed\",lw = 0.5)\n",
    "plt.vlines(1.8, 0, 0.4,colors = \"r\", linestyles = \"dashed\",lw = 0.5)\n",
    "plt.vlines(1.4, 0.62, 1,colors = \"r\", linestyles = \"dashed\",lw = 0.5)\n",
    "plt.hlines(0.5,0,1.6129,colors = \"r\", linestyles = \"dashed\",lw = 0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图中,橙色水平线为在训练集中不是V型的花瓣宽度, 蓝色水平线为是V型的花瓣宽度,可以看到二者有重叠   \n",
    "经训练后模型给出决策边界在两条曲线相交处50%对应的1.6129为界   \n",
    "在二维特征中,决策边界可能为条带区域   \n",
    "在sklearn中,$logistic\\ regression\\ cost\\ function$ 默认加$l2$范数罚   \n",
    "同时不同于其他模型,规定正则化程度的是$C$而非常见的$alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Logistics\\ Regression$不必像二分类器那样组合形成多标签分类器    \n",
    "称支持多$class$的$logistics\\ regression$为$Softmax\\ Regression$或$Multinomial\\ Logistic Regression$\n",
    "\n",
    "- $Softmax\\ function$    \n",
    "柔性最大值函数, 归一化指数函数   \n",
    "是逻辑函数的推广,Logistics将实数集映射到$[0,1]$,   \n",
    "$softmax$函数将任意实数的$K$维向量$\\mathbf{z}$压缩到另一个$K$维实向量$\\sigma(\\mathbf{z})$中,使得每一个元素都在$(0,1)$,且元素之和为$1$    \n",
    "该函数通常为以下形式:    \n",
    "$\\displaystyle \\sigma(\\mathbf{z})_j=\\frac{e^{\\mathit{z}_j}}{\\sum_{k=1}^{K}e^{\\mathit{z}_k}}$,    $j=1,2,..,K $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $Softmax\\ Regression$多标签分类实现    \n",
    "<br>\n",
    "对于一个样例$\\mathbf{x}$计算出每一个$class$的$probability$ $s_k(\\mathbf{x}),k=1,...,K$ 放进$\\sigma(\\cdot)$中得到$\\hat{p}_k$   \n",
    "其中$s_k\\mathbf(\\mathbf{x})=\\theta_k^T\\cdot \\mathbf{x}$, $\\theta_k$为第$k$个$class$对应的参数    \n",
    "$\\displaystyle \\hat{p}_k=\\sigma(\\mathbf{s}(\\mathbf{x}))_k=\\frac{e^{s_k(\\mathbf{x})}}{\\sum_{j=1}^{K}e^{s_j(\\mathbf{x})}}$ \n",
    "\n",
    "> - $K$ $class$数量   \n",
    "- $\\mathbf{s}(\\mathbf{x})$  $vector$ 所有$class$求得$softmax\\ score$, $\\mathbf{s}(\\mathbf{x})=\\begin{pmatrix}\n",
    " \\theta_1^T\\cdot \\mathbf{x}\\\\ \n",
    "\\cdot \\\\\n",
    "\\cdot \\\\\n",
    "\\theta_K^T\\cdot \\mathbf{x}\\\\ \n",
    "\\end{pmatrix}$\n",
    "  \n",
    "注意Softmax分类器以多项式分布为模型,其可以分类的是多种互斥的类别    \n",
    "$\\displaystyle \\hat{y}=\\underset{k}{argmax} \\ \\hat{p}_k$等价于 $\\underset{k}{argmax}(\\theta_k^T \\cdot \\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Softmax Regression\n",
    "最小化成本函数, 在多标签问题中我们采用**交叉熵**评估参数    \n",
    "- $\\displaystyle J(\\Theta)=-\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^{K}1(y_k^{(i)}=k)log(\\hat{p}^{(i)}_k)$    \n",
    "当模型预测$\\hat{y}=k时我们加上-log(\\hat{p}_k)$作为惩罚项   \n",
    "当$K=2$时正好为$log\\ loss$   \n",
    "\n",
    "**交叉熵**    \n",
    "在两个离散型概率分布$p,q$,定义交叉熵$H(p,q)=-\\sum_xp(x)log(q(x))$     \n",
    "$\\displaystyle \\bigtriangledown \\theta_kJ(\\Theta)=-\\frac{1}{m}\\sum_{i=1}^{m}(1(y_k^{(i)}=k)-\\hat{p}_k^{(i)})\\mathbf{x}^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris['data'][:, (2,3)]\n",
    "y = iris['target']\n",
    "softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)\n",
    "softmax_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), array([[6.33134076e-07, 5.75276067e-02, 9.42471760e-01]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict([[5, 2]]), softmax_reg.predict_proba([[5, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 核外算法,例如SGD,Mini-Batch GD\n",
    "2. GD,尤其是正则化过的线性模型, feature scaling\n",
    "3. NO, Logistic Regression 对应的log loss是凸函数不存在local minimum\n",
    "4. NO, 除了全批量,其余的GD算法最优解在Minimum附近震荡\n",
    "5. NO, 过拟合(training error not up)或者学习率过高学得不细\n",
    "6. NO, Mini-Batch GD和SGD都是随机算法,到达Minimum前的路是震荡的,我们能做的是,在每个epoch时存上最好的模型\n",
    "7. SGD, BGD, 对于随机GD, 越往后学习率越低有利于震荡在Minimum\n",
    "8. 过拟合, 增加训练数据, 减少模型复杂度(degree), 正则化来约束模型自由度\n",
    "9. 高偏差, 降低$\\alpha$   \n",
    "10. (1) 高偏差, 误差降不下来, 模型自由度过高, 总之正则化的模型表现得更为优异   \n",
    "    (2) 需要加上稀疏约束,  自动特征选择   \n",
    "    (3) 某些特征存在较强的关联性, 或特征数明显小于样本数   \n",
    "11. 多输出, logistics regression每次只能归一类,不是多输出模型,故需要两个二分类器处理多输出多标签分类问题, 或者组合标签又显得较为复杂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 手动实现带停止法的全批量更新Softmax Regression模型   \n",
    "数据集用Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.16 ms, sys: 1.47 ms, total: 4.63 ms\n",
      "Wall time: 3.72 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ============================================================ #\n",
    "# implement Softmax Regression based BGD with Earning Stopping #\n",
    "# TODO: [X] data clean                                         #\n",
    "# TODO: [X] One hot encoding                                   #\n",
    "# TODO: [X] Softmax function                                   #\n",
    "# TODO: [X] Ridge Regression                                   #\n",
    "# TODO: [X] Training models                                    #\n",
    "# TODO: [X] Validating models                                  #\n",
    "# TODO: [X] Earning Stopping                                   #\n",
    "# ============================================================ #\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "raw_data = load_iris()\n",
    "# print(raw_data.keys())\n",
    "X = raw_data['data'][:, (2, 3)]  # petal length and width\n",
    "y = raw_data['target']\n",
    "\n",
    "# add x_0=1\n",
    "X = np.c_[np.ones([len(X), 1]), X]  # ones->shape (len(X), 1)\n",
    "\n",
    "# ============================================================\n",
    "# split train set and test set\n",
    "test_ratio, validation_ratio, tot_size = 0.2, 0.2, len(X)\n",
    "test_size, validation_size = int(tot_size * test_ratio), int(tot_size * validation_ratio)\n",
    "train_size = tot_size - test_size - validation_size\n",
    "\n",
    "random_idx = np.random.permutation(tot_size)\n",
    "X_train = X[random_idx[:train_size]]\n",
    "y_train = y[random_idx[:train_size]]\n",
    "\n",
    "X_valid = X[random_idx[train_size:-test_size]]\n",
    "y_valid = y[random_idx[train_size:-test_size]]\n",
    "\n",
    "X_test = X[random_idx[-test_size:]]\n",
    "y_test = y[random_idx[-test_size:]]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# y = {0, 1, 2}\n",
    "# we have three class, so let do onehot for them\n",
    "# if n_class=2 it's y look like [0, 1, 0]\n",
    "\n",
    "def de_one_hot(Y):\n",
    "    n_classes = Y.max() + 1\n",
    "    m = len(Y)\n",
    "    y_one_hot = np.zeros((m, n_classes))\n",
    "    y_one_hot[np.arange(m), Y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "\n",
    "y_train = de_one_hot(y_train)\n",
    "y_valid = de_one_hot(y_valid)\n",
    "y_test = de_one_hot(y_test)\n",
    "\n",
    "# ============================================================\n",
    "# Softmax function\n",
    "\n",
    "def softmax(sx):\n",
    "    \"\"\"\n",
    "    :param sx: All classes softmax score matrix\n",
    "    :return p: ever class probability\n",
    "    \"\"\"\n",
    "    exps = np.exp(sx)\n",
    "    sum_exps = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / sum_exps\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# define number of in/output\n",
    "\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y))\n",
    "# Theta :(3 * 3) 3 class, 3 features\n",
    "\n",
    "eta = 0.01\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7  # avoid getting nan value when cal log(pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0000 <==================> loss: 1.469604293745519\n",
      "Epoch-0500 <==================> loss: 0.5546031362623158\n",
      "Epoch-1000 <==================> loss: 0.49755344752576314\n",
      "Epoch-1500 <==================> loss: 0.45802641691452484\n",
      "Epoch-2000 <==================> loss: 0.42840128032832586\n",
      "Epoch-2500 <==================> loss: 0.4049577455578595\n",
      "Epoch-3000 <==================> loss: 0.38568313841098867\n",
      "Epoch-3500 <==================> loss: 0.3693910820583783\n",
      "Epoch-4000 <==================> loss: 0.355331726017379\n",
      "Epoch-4500 <==================> loss: 0.34300359813216036\n",
      "Epoch-5000 <==================> loss: 0.3320556275914768\n",
      "Scores: 0.9555555555555556\n",
      "CPU times: user 239 ms, sys: 7.95 ms, total: 247 ms\n",
      "Wall time: 244 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ============================================================\n",
    "# training and validating model\n",
    "\n",
    "Thetas = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    sk = X_train.dot(Thetas)\n",
    "    pk = softmax(sk)\n",
    "    log_loss = -np.mean(np.sum(y_train * np.log(pk + epsilon), axis=1))\n",
    "    error = pk - y_train\n",
    "    if not iteration % 500:\n",
    "        print('Epoch-{} <==================> loss: {}'.format(str(iteration).zfill(4), log_loss))\n",
    "    gradients = 1 / m * X_train.T.dot(error)\n",
    "    Thetas = Thetas - eta * gradients\n",
    "\n",
    "vsk = X_valid.dot(Thetas)  # validating set sk\n",
    "vpk = softmax(vsk)  # softmax sk then output pk\n",
    "vy_predict = de_one_hot(np.argmax(vpk, axis=1))  # 输出的是(1, -1) 转换成(-1, 3)\n",
    "print('Scores:', np.mean(vy_predict == y_valid))  # compare with validation set\n",
    "# 0.9555555555555556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0000 <==================> l2 loss: 4.188061038052556\n",
      "Epoch-0500 <==================> l2 loss: 0.9170325188465561\n",
      "Epoch-1000 <==================> l2 loss: 0.7597808874896241\n",
      "Epoch-1500 <==================> l2 loss: 0.6769156957525758\n",
      "Epoch-2000 <==================> l2 loss: 0.6280160467591782\n",
      "Epoch-2500 <==================> l2 loss: 0.5964058949371841\n",
      "Epoch-3000 <==================> l2 loss: 0.5744852332749585\n",
      "Epoch-3500 <==================> l2 loss: 0.5584507279861403\n",
      "Epoch-4000 <==================> l2 loss: 0.5462369782175306\n",
      "Epoch-4500 <==================> l2 loss: 0.5366405579188294\n",
      "Epoch-5000 <==================> l2 loss: 0.5289169406470305\n",
      "Scores: 0.9555555555555556\n",
      "CPU times: user 428 ms, sys: 7.02 ms, total: 435 ms\n",
      "Wall time: 444 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ============================================================\n",
    "# try l2 penalty to regularize and validating model\n",
    "\n",
    "Thetas = np.random.randn(n_inputs, n_outputs)\n",
    "alpha = 0.1  # hyperparameter control l2 penalty\n",
    "for iteration in range(n_iterations):\n",
    "    sk = X_train.dot(Thetas)\n",
    "    pk = softmax(sk)\n",
    "    xcross_loss = -np.mean(np.sum(y_train * np.log(pk + epsilon), axis=1))  # cal Cross entropy loss\n",
    "    l2_loss = xcross_loss + alpha * 0.5 * np.sum(np.square(Thetas[1:]))  # cal l2 loss\n",
    "    error = pk - y_train\n",
    "    if not iteration % 500:\n",
    "        print('Epoch-{} <==================> l2 loss: {}'.format(str(iteration).zfill(4), l2_loss))\n",
    "    gradients = 1 / m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Thetas[1:]]\n",
    "    Thetas -= gradients * eta\n",
    "\n",
    "vsk = X_valid.dot(Thetas)  # validating set sk\n",
    "vpk = softmax(vsk)  # softmax sk then output pk\n",
    "vy_predict = de_one_hot(np.argmax(vpk, axis=1))  # 输出的是(1, -1)转换成(-1, 3)\n",
    "print('Scores:', np.mean(vy_predict == y_valid))  # compare with validation set\n",
    "# 0.9777777777777777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0000 <==================> l2 loss: 2.483870190039182\n",
      "Epoch-0500 <==================> l2 loss: 0.8486960759800519\n",
      "Epoch-1000 <==================> l2 loss: 0.6962784649905293\n",
      "Epoch-1500 <==================> l2 loss: 0.6231152746231722\n",
      "Epoch-2000 <==================> l2 loss: 0.5840177019388\n",
      "Epoch-2500 <==================> l2 loss: 0.5606858240218848\n",
      "Epoch-3000 <==================> l2 loss: 0.5453307739345411\n",
      "Epoch-3500 <==================> l2 loss: 0.5344157788724228\n",
      "Epoch-4000 <==================> l2 loss: 0.5262058202640714\n",
      "Epoch-4500 <==================> l2 loss: 0.519775552157043\n",
      "Epoch-5000 <==================> l2 loss: 0.5145896830776464\n",
      "Scores: 0.9777777777777777\n",
      "CPU times: user 430 ms, sys: 8.17 ms, total: 438 ms\n",
      "Wall time: 447 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ============================================================\n",
    "# Earning Stopping\n",
    "\n",
    "best_loss = np.infty\n",
    "Thetas = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    sk = X_train.dot(Thetas)\n",
    "    pk = softmax(sk)\n",
    "    xcross_loss = -np.mean(np.sum(y_train * np.log(pk + epsilon), axis=1))  # cal Cross entropy loss\n",
    "    l2_loss = xcross_loss + alpha * 0.5 * np.sum(np.square(Thetas[1:]))  # cal l2 loss\n",
    "    error = pk - y_train\n",
    "    gradients = 1 / m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Thetas[1:]]\n",
    "    Thetas -= gradients * eta\n",
    "\n",
    "    if not iteration % 500:\n",
    "        print('Epoch-{} <==================> l2 loss: {}'.format(str(iteration).zfill(4), l2_loss))\n",
    "    if l2_loss < best_loss:\n",
    "        best_loss = l2_loss\n",
    "    else:\n",
    "        print('Epoch-{} <==================> l2 loss: {}'.format(str(iteration - 1).zfill(4), best_loss))\n",
    "        print('@Earning Stopping!@')\n",
    "        break\n",
    "\n",
    "vsk = X_valid.dot(Thetas)  # validating set sk\n",
    "vpk = softmax(vsk)  # softmax sk then output pk\n",
    "vy_predict = de_one_hot(np.argmax(vpk, axis=1))  # 输出的是(1, -1)转换成(-1, 3)\n",
    "print('Scores:', np.mean(vy_predict == y_valid))  # compare with validation set\n",
    "# 0.9555555555555556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Scores: 1.0\n",
      "CPU times: user 506 µs, sys: 312 µs, total: 818 µs\n",
      "Wall time: 580 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ============================================================\n",
    "# Test in test set\n",
    "\n",
    "tsk = X_test.dot(Thetas)\n",
    "tpk = softmax(tsk)\n",
    "ty_predict = de_one_hot(np.argmax(tpk, axis=1))\n",
    "print('Final Scores:', np.mean(ty_predict == y_test))\n",
    "# 0.9777777777777777\n",
    "\n",
    "# ============================================================\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
