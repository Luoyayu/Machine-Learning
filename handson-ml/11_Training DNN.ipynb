{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Xavier and He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Datasets => model => loss => train => eval => Session().train+eval+test+log\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import numpy as np\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_output = 10\n",
    "n_epoch = 40\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    hidden1 = tf.layers.dense(inputs=X, units=n_hidden1, activation=tf.nn.selu, kernel_initializer=he_init,name='hidden1')\n",
    "    hidden2 = tf.layers.dense(inputs=hidden1,units=n_hidden2,activation=tf.nn.selu,kernel_initializer=he_init,name='hidden2')\n",
    "    logists = tf.layers.dense(inputs=hidden2,units=n_output,name = 'output')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logists)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')    \n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(predictions=logists, targets=y, k=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))  \n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, n_inputs) / 255.\n",
    "X_test  = X_test.astype(np.float32).reshape(-1, n_inputs) / 255.\n",
    "y_train, y_test = y_train.astype(np.int32), y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch        \n",
    "        \n",
    "X_train_means = X_train.mean(axis=0, keepdims=True)\n",
    "X_train_stds = X_train.std(axis=0, keepdims=True) + 1e-12\n",
    "X_val_scaled = (X_valid - X_train_means) / X_train_stds\n",
    "X_test_scaled = (X_test - X_train_means) / X_train_stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epoch):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - X_train_means) / X_train_stds\n",
    "            sess.run(training_op, feed_dict={X:X_batch_scaled, y:y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X:X_batch_scaled, y:y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X:X_val_scaled, y:y_valid})\n",
    "            print(f'{epoch} Batch acc: {acc_batch}; Valid acc: {acc_valid}')\n",
    "    acc_test = accuracy.eval(feed_dict={X:X_test_scaled, y:y_test})       \n",
    "    print(f'Test acc: {acc_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\"使用TF dataset APIs 并行处理数据和计算，替代传统的feed-dict方式\"\n",
    "# importing Data => create Iterator => consuming Data\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time, os, sys\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "n_input = 28 * 28\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype(np.float32).reshape(-1, n_input)\n",
    "X_test = X_test.astype(np.float32).reshape(-1, n_input)\n",
    "\n",
    "y_train, y_test = y_train.astype(np.int32), y_test.astype(np.int32)\n",
    "datasetX, datasety = np.r_[X_train, X_test], np.r_[y_train, y_test]\n",
    "\n",
    "\"从numpy中获取数据\"\n",
    "dataset = tf.data.Dataset.from_tensor_slices((datasetX, datasety))\n",
    "if False:\n",
    "    dataset\n",
    "\n",
    "\"四种迭代器\"\n",
    "\"1. One shot迭代器\"\n",
    "iter = dataset.make_one_shot_iterator(), \"call get_next() to use dateset\"\n",
    "el = iter.get_next()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\"2. 可初始化迭代器\"\n",
    "X = tf.placeholder(dtype=np.float32, shape=(None, n_input), name='X')\n",
    "y = tf.placeholder(dtype=np.int32, shape=(None), name='y')\n",
    "\n",
    "\"从占位符中获取数据，使用可初始化迭代器喂数据\"\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "init_iter = dataset.make_initializable_iterator()\n",
    "features, labels = init_iter.get_next()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_iter.initializer, feed_dict={X: datasetX, y:datasety}) # 初始化 迭代器\n",
    "    mnist_img, minst_label = sess.run([features, labels]) # 迭代一次 迭代器\n",
    "    plt.imshow(X=mnist_img.reshape(28,28), cmap='gray')\n",
    "    print(f'label of following mnist image is {minst_label}')\n",
    "    \n",
    "\"3. 可重初始化的迭代器\"\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(dtype=np.float32, shape=(None, n_input), name='X')\n",
    "y = tf.placeholder(dtype=np.int32, shape=(None), name='y')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "# test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "\"创建通用性迭代器，该迭代器无initializer方法，预初始化迭代器应使用Iterator.make_initializer(dataset)生成可重用迭代器\"\n",
    "iter = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n",
    "\n",
    "features, labels = iter.get_next() # 张量生成器\n",
    "\n",
    "\"多次初始化 init_op\"\n",
    "train_init_op = iter.make_initializer(dataset)\n",
    "# test_init_op  = iter.make_initializer(test_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_init_op, feed_dict={X:X_train, y:y_train}) # 切换到训练数据集\n",
    "    mnist_img, minst_label = sess.run([features, labels])\n",
    "#     plt.imshow(X=mnist_img.reshape(28,28), cmap='gray')\n",
    "    print(f'label of following Train mnist image is {minst_label}')\n",
    "    assert(minst_label == 5)\n",
    "#     plt.show()\n",
    "    \n",
    "    sess.run(train_init_op, feed_dict={X:X_test, y:y_test}) # 切换到测试数据集\n",
    "    mnist_img, minst_label = sess.run([features, labels])\n",
    "#     plt.imshow(X=mnist_img.reshape(28,28), cmap='gray')\n",
    "    print(f'label of following Test mnist image is {minst_label}')\n",
    "    assert(minst_label == 7)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch acc is 0.96875; Val acc is 0.9168000221252441\n",
      "5 Batch acc is 0.953125; Val acc is 0.9513999819755554\n",
      "10 Batch acc is 0.96875; Val acc is 0.9624000191688538\n",
      "15 Batch acc is 0.984375; Val acc is 0.9648000001907349\n",
      "20 Batch acc is 1.0; Val acc is 0.9666000008583069\n",
      "25 Batch acc is 1.0; Val acc is 0.9675999879837036\n",
      "30 Batch acc is 1.0; Val acc is 0.9684000015258789\n",
      "35 Batch acc is 1.0; Val acc is 0.9693999886512756\n",
      "Test acc is 0.972000002861023\n",
      "CPU times: user 9min 30s, sys: 35 s, total: 10min 5s\n",
      "Wall time: 4min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tf.data 全数据 batch手动正则化\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "n_input = 28 * 28\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, n_input) / 255.\n",
    "X_test  = X_test.astype(np.float32).reshape(-1, n_input) / 255.\n",
    "y_train ,y_test= y_train.astype(np.int32), y_test.astype(np.int32)\n",
    "X_val, X_train = X_train[:5000], X_train[5000:]\n",
    "y_val, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_output = 10\n",
    "lr = 0.01\n",
    "n_epoch = 40\n",
    "batch_size = 64\n",
    "n_batch = X_train.shape[0] // batch_size\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_input), name='X')\n",
    "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    hidden1 = tf.layers.dense(inputs=X,       units=n_hidden1, activation=tf.nn.selu, kernel_initializer=he_init, name='hidden1')\n",
    "    hidden2 = tf.layers.dense(inputs=hidden1, units=n_hidden2, activation=tf.nn.selu, kernel_initializer=he_init, name='hidden2')\n",
    "    logits  = tf.layers.dense(inputs=hidden2, units=n_output,                                                     name='logits')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(predictions=logits, targets=y, k=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(batch_size).shuffle(X_train.shape[0]) # 无限随机生成\n",
    "iter = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes) # 可重初始化的迭代器\n",
    "features, labels = iter.get_next() # 张量生成器\n",
    "init_op = iter.make_initializer(dataset) # 迭代器初始化操作\n",
    "\n",
    "X_train_means = X_train.mean(axis=0, keepdims=True)\n",
    "X_train_stds = (X_train.std(axis=0, keepdims=True) + 1e-12)\n",
    "data_scaled = lambda _:(_ - X_train_means) / X_train_stds\n",
    "X_test_scaled, X_val_scaled = data_scaled(X_test), data_scaled(X_val)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epoch):  \n",
    "        sess.run(init_op, feed_dict={X:X_train, y:y_train}) # 从 X_train 中初始化迭代器\n",
    "        for _ in range(n_batch):\n",
    "            features_, labels_ = sess.run([features, labels])\n",
    "            features_scaled = data_scaled(features_) # 不在外面正则化X_train，选在在epoch中batch_size规模计算\n",
    "            sess.run(training_op, feed_dict={X:features_scaled, y:labels_}) \n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X:features_scaled, y:labels_})\n",
    "            acc_valid = accuracy.eval(feed_dict={X:X_val_scaled, y:y_val})\n",
    "            print(f'{epoch} Batch acc is {acc_batch}; Val acc is {acc_valid}')  \n",
    "    acc_test = accuracy.eval(feed_dict={X:X_test_scaled, y:y_test})\n",
    "    print(f'Test acc is {acc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELU, ELU, Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用BN通常不用再正则化训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "training = tf.placeholder_with_default(input=False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(inputs=X, units=n_hidden1, name='hidden1')\n",
    "bn1 = tf.layers.batch_normalization(inputs=hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1) # 在激活函数前完成BN层\n",
    "# 重复为hidden2完成BN\n",
    "hidden2 = tf.layers.dense(inputs=bn1_act, units=n_hidden2, name='hidden2')\n",
    "bn2 = tf.layers.batch_normalization(inputs=hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2) # 在激活函数前完成BN层\n",
    "# 重复为Softmax分类层完成BN\n",
    "logists_before_bn = tf.layers.dense(inputs=bn2_act, units=n_outputs, name='logists')\n",
    "logists = tf.layers.batch_normalization(inputs=logists_before_bn, training=training, momentum=0.9)\n",
    "# 最后一层无需激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "hidden1 = tf.layers.dense(inputs=X, units=n_hidden1, name='hidden1')\n",
    "bn1 = batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(inputs=bn1_act, units=n_hidden2, name='hidden2')\n",
    "bn2 = batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logists_before_bn = tf.layers.dense(inputs=bn2_act, units=n_outputs, name='logists')\n",
    "logists = batch_norm_layer(logists_before_bn)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
